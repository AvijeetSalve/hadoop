sudo yum update
sudo yum install java-1.8.0-openjdk-src.x86_64
sudo yum install java-devel
sudo alternatives --config java
sudo sh -c "echo export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.362.b09-4.el9.x86_64/jre >> /etc/envirnoment"
sudo yum install wget
sudo yum install openssh-server
sudo systemctl status sshd
sudo systemctl start sshd
sudo systemctl enable sshd
sudo yum install vim
##Download the psdh tarball from this url https://code.google.com/archive/p/pdsh/downloads
##Go to the location where the tar file is! then unzip and untar it!
bunzip2 pdsh-2.28.tar.bz2
tar -xvf pdsh-2.28.tar
sudo yum install autoconf.noarch
sudo yum install gcc
#Go to the extracted file:
./configure
sudo make 
sudo make check 
sudo make install
sudo useradd hduser
sudo passwd hduser
sudo visudo
#In this file add :
hduser  ALL=(ALL) NOPASSWD:ALL
#Then go to hduser
su - hduser
#Inside hduser
sudo hostnamectl set-hostname NameNode
sudo hostname NameNode

#####Add network adapters to the virtual machine application and add those to all the virtual machines you want to keep in the cluster
#####Then go inside each VM and connect to the enp0s8 wired connection which will enable ip addresses to all the virtual machines




##Then edit the .bashrc file
sudo vi ~/.bashrc
##In this file write this given code before "unset rc":-
#JAVA
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/
export JRE_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre
#Hadoop Environment Variables
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
export HADOOP_LOG_DIR=$HADOOP_HOME/logs
export HADOOP_MAPRED_HOME=$HADOOP_HOME
# Add Hadoop bin/ directory to PATH
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
export PDSH_RCMD_TYPE=ssh

sudo source ~/.bashrc
cd
wget https://dlcdn.apache.org/hadoop/common/hadoop-3.2.4/hadoop-3.2.4.tar.gz
sudo mkdir /usr/local/hadoop
tar -xzvf hadoop-3.2.4.tar.gz
sudo mv hadoop-3.2.4/* /usr/local/hadoop
sudo chown -R hduser:hduser /usr/local/hadoop
sudo chmod 755 -R /usr/local/hadoop
sudo find / -name java ##Find the java file which is in the bin of a file then copy the path and paste that path in .bashrc and hadoop-env.sh
###
For eg. this files
/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.362.b09-4.el9.x86_64/jre/bin/java
###



#Do these 2 steps on all the nodes:-
sudo systemctl stop firewalld.service
sudo systemctl disable firewalld.service





#####now edit the hadoop-env.sh by going to /usr/local/hadoop/etc/hadoop and paste the code:-
########################################################################################
#JAVA
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/
export JRE_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre
#Hadoop Environment Variables
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
export HADOOP_LOG_DIR=$HADOOP_HOME/logs
export HDFS_NAMENODE_USER=hduser
export HDFS_DATANODE_USER=hduser
export HDFS_SECONDARYNAMENODE_USER=hduser
export YARN_RESOURCEMANAGER_USER=hduser
export YARN_NODEMANAGER_USER=hduser
export YARN_NODEMANAGER_USER=hduser
# Add Hadoop bin/ directory to PATH
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
########################################################################################



#####now edit the core-site.sh by going to /usr/local/hadoop/etc/hadoop and paste the code:-
########################################################################################
<property>
        <name>fs.defaultFS</name>
        <value>hdfs://Namenode:9000</value>
    </property>
########################################################################################



#####now edit the hdfs-site.sh by going to /usr/local/hadoop/etc/hadoop and paste the code:-
########################################################################################
<property>
        <name>dfs.name.dir</name>
        <value>/usr/local/hadoop/hd-data/nn</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
<property>
        <name>dfs.hosts.exclude</name>
        <value>/usr/local/hadoop/etc/hadoop/dfs.hosts.exclude</value>
    </property>
<property>
        <name>dfs.namenode.acls.enabled</name>
        <value>true</value>
    </property>
########################################################################################
(now add dfs.hosts.enabled file and write the name of the node you want to decomission) 



#####now edit the mapred-site.sh by going to /usr/local/hadoop/etc/hadoop and paste the code:-
########################################################################################
<property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.application.classpath</name>
        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
    </property>
#######################################################################################





#####now edit the yarn-site.sh by going to /usr/local/hadoop/etc/hadoop and paste the code:-
########################################################################################
<property>
        <name>yarn.resourcemanager.hostname</name>
        <value>NameNode</value>
    </property>
#######################################################################################



#####now edit the workers by going to /usr/local/hadoop/etc/hadoop and paste the code:-
########################################################################################
datanode name
datanode name
#######################################################################################

-------------------------------------------------------------------------------------------------------------------------------------------------------------------
######################################################################################
Now clone the numbers of nodes you want but then edit files:-
                                                        core-site.xml
                                                        hdfs-site.xml
                                                        mapred-site.xml
                                                        yarn-site.xml
in all the datanodes
###
In datanode, core-site.xml:-
<property>
        <name>fs.defaultFS</name>
        <value>hdfs://NameNode:9000</value>
    </property>


In datanode, hdfs-site.xml:-
<property>
        <name>dfs.data.dir</name>
        <value>/usr/local/hadoop/hd-data/dn</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>


In datanode, mapred-site.xml:-
<property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.application.classpath</name>
        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
    </property>


In datanode, yarn-site.xml:-
<property>
        <name>yarn.resourcemanager.hostname</name>
        <value>NameNode</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.local-dirs</name>
        <value>/usr/local/hadoop/hd-data/yarn/data</value>
    </property>
    <property>
        <name>yarn.nodemanager.logs-dirs</name>
        <value>/usr/local/hadoop/hd-data/yarn/logs</value>
    </property>
    <property>
        <name>yarn.nodemanager.disk-health-checker.max-disk-utilization-perdisk-percentage</name>
        <value>99.9</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>

In datanode, all nodes workers file will have:-
localhost 








ssh-copy-id -i .ssh/id_rsa.pub hduser@DN3


scp /usr/local/hadoop/etc/hadoop/hadoop-env.sh  hduser@DN2:/usr/local/hadoop/etc/hadoop/hadoop-env.sh 
scp .bashrc hduser@DN3:/home/hduser/.bashrc














hadoop namenode -format
start-dfs.sh
start-yarn.sh
hdfs dfs -ls /
hdfs dfs -mkdir /dirname
hdfs dfs -touch /dirname/filename
hdfs dfs -put filename /dir
hdfs dfsadmin -report
hdfs dfsadmin -refreshNodes
hdfs dfsadmin -printTopology
